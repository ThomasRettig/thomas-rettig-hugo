<!DOCTYPE html>
<html><head>
    <html lang="en">
    <meta charset="utf-8">
    <meta name="author" content="Thomas Rettig">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="The personal blog of Thomas Rettig.">
    
    <link rel="shortcut icon" href="https://thms.netlify.app/favicon.ico">
    
    <link rel="stylesheet" href="/css/style.min.css">

    <title>Is word count really accurate?</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
</head>
<body><header id="banner">
    <a title="homepage" href="https://thms.netlify.app/">th[o]m[a]s</a>
    <nav>
        <ul>
            <li>
                <a href="/about/" title="about me">about</a>
            </li><li>
                <a href="/contact/" title="contact me">contact</a>
            </li>
        </ul>
    </nav>
</header>
<main id="content">
<article>
    <header id="post-header">
        <h1>Is word count really accurate?</h1>
            <div>
                <share title="Share this article" class="share">Share this article</share><time>November 10, 2021</time>
                </div>
    </header><p>Recently, I’ve asked myself this question: <em>Is there a better way to calculate essay length than word count?</em> Just wonderin’. While word count might work fine for a single-page documents, word count becomes less accurate for hundred-page documents, like, um, a PhD thesis.</p>
<h2 id="the-problem">The problem</h2>
<p>So, here’s the thing. In linguistics, <a href="https://en.wikipedia.org/wiki/Agglutinative_language">agglutinative languages</a> exist. In other words:</p>
<blockquote>(of a language, e.g. Hungarian, Turkish, Korean, and Swahili) tending to express concepts in complex words consisting of many elements, rather than by inflection or by using isolated elements.
</blockquote>
<figcaption>—Oxford English Dictionary</figcaption>
<p>What this basically means is that some languages have <em>longer</em> words, while others have <em>shorter</em> words. A good example would be Turkish. Like so:</p>
<figure><img src="https://res.cloudinary.com/cloudinary-sucks-so-fucking-much/image/upload/v1636555029/agglutinative-language-upscaled.webp"
         alt="Comparison of an agglutinative language with a non-agglutinative language"/><figcaption>
            <h4>Comparison of an agglutinative language with a non-agglutinative language</h4><p><br>Note: This is an upscaled version generated by <a href="https://github.com/AaronFeng753/Waifu2x-Extension-GUI">Waifu2x</a></p>
        </figcaption>
</figure>

<p>A single word in Turkish could make for an entire sentence in English, because each word has, well, more “words” in it. Thus, I feel that it’s rather inaccurate to judge document length just like that. Yes, in technical terms, word count <em>does</em> equal to the byte size of a digital document, but from the aspect of “how much is this person really saying”, word count scores poorly.</p>
<h2 id="existing-solutions">Existing solutions</h2>
<p>An interesting point of research is linguist Joseph Greenberg’s work in measuring the “agglutinativeness” of a language. In his <a href="https://www.jstor.org/stable/1264155?read-now=1&amp;refreqid=excelsior%3A5f06e207ebd245aaf83c24e72c55976c">academic paper</a>, written in 1960 (though it’s still relevant), the degree of agglutinativeness is based on a ratio: The number of agglutinative junctures to the number of morph junctures. It’s a bit over my head, but the evidence is in the table below, extracted from Luschützky (2003):</p>
<table>
<tbody>
<tr>
<td></td>
<th>Agglutination</th>
<th>Synthesis</th>
</tr>
<tr>
<th>Swahili</th>
<td>0.67</td>
<td>2.56</td>
</tr>
<tr>
<th>Turkish</th>
<td>0.60</td>
<td>2.33</td>
</tr>
<tr>
<th>Yakut</th>
<td>0.51</td>
<td>2.17</td>
</tr>
<tr>
<th>English</th>
<td>0.30</td>
<td>1.67</td>
</tr>
</tbody>
</table>
<p>Solely from the agglutination index, there is a difference 0.37 between the agglutinative Swahili and the non-agglutinative English. So, what does that mean? I suppose it works like this: A text written in Swahili can fit in almost two times more content than a text written in English. So is word count really accurate? Is it really a universal marker of text length? Let’s not even get into Mandarin, which itself is perhaps one of the most agglutinative languages in the world, which was, for whatever reason, seemingly omitted from the aforementioned study.</p>
<h2 id="personal-thoughts">Personal thoughts</h2>
<p>Based on the above, how then, do we measure document length more accurately? I have a theory. Based on the assumption that clause-binding words/phrases such as “therefore”, “so”, “hence”, “as a result” indicate a certain degree of logical advancement in content, we could instead count these words/phrases in a text such that a greater number would result in a higher “content length” value.</p>
<p>But wait—we can go even further! What if we could leverage an unsupervised <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> model to understand the “content length”? This is a super interesting idea, and could actually be useful. The bottom line: Word count varies from language to language, and while it has its conveniences and relevance, more robust systems for calculating document length need to be formed.</p>
</article>

        <hr>
        </main><footer id="footer">
    Copyright © 2021 Thomas
</footer>
<script>
        const shareButton = document.querySelector('share')
        const shareByline = ' by Thomas Rettig'
        const title = window.document.title
        const url = window.document.location.href
    
    shareButton.addEventListener('click', event => {

    if (navigator.share) {
        navigator.share({
            title: `“${title}”` + `${shareByline}`,
            url: `${url}`,
        })
        } else {
                overlay.classList.add('show-share')
                shareModal.classList.add('show-share')
        }
        
    });
</script></body>
</html>
